## 图神经网络

Graph Neural Network，GNN

### 概述

图（Graph）数据包含着十分丰富的关系型信息。从文本、图像这些非结构化数据中进行推理学习，例如句子的依赖树、图像的场景图等，都需要图推理模型。图神经网络（Graph neural networks）是一种链接主义模型，它靠图中节点之间的信息传递来捕捉图中的依赖关系。

图是一种结构化数据，它由一系列的对象（nodes）和关系类型（edges）组成。作为一种非欧几里得形数据，图分析被应用到节点分类、链路预测和聚类等方向。图网络是一种基于图域分析的深度学习方法。

1. 卷积神经网络（CNN）是GNN起源的首要动机。
CNN有能力去抽取多尺度局部空间信息，并将其融合起来构建特征表示。CNN只能应用于常规的欧几里得数据上（例如2-D的图片、1-D的文本），这些形式的数据可以被看成是图的实例化。

随着对GNN和CNN的深入分析，发现其有三个共同的特点：（1）局部连接（2）权值共享（3）多层网络。

这对于GNN来说同样有重要的意义。（1）局部连接是图的最基本的表现形式。（2）权值共享可以减少网络的计算量。（3）多层结构可以让网络捕获不同的特征。然而，从CNN到GNN的转变还面临这另一个问题，难以定义局部卷积核和池化操作。这也阻碍了CNN由传统欧几里得空间向非欧几里得空间的扩展。

2. 这另一个动机就是图表示（graph embedding），即如何将图中的节点、边和子图以低维向量的形式表现出来。
受启发于表示学习（representation learning）和词嵌入（word embedding），图嵌入技术得到了长足的发展。DeepWalk基于表示学习，被认为是第一种图嵌入技术模型。还有基于SkipGram模型的random walk。以及node2vec，LINE和TADW等。然而以上的这些模型存在两个缺点：（1）图中节点之间不存在任何的参数共享，导致计算量与节点数量呈线性增长。（2）图嵌入技术缺乏泛化能力，导致不能处理动态图或推广至新的图。

GNN相对于传统的神经网络来说也是存在优势的。标准的CNN和RNN网络不能处理图输入这种非顺序排序的特征表示。换句话说，图中节点的排序是没有规律可言的。如果非要用传统的CNN和RNN来处理图数据的话，只能遍历图中节点所有可能的出现顺序作为模型的输入，这对模型的计算能力来说是难以承受的。为了解决这个问题，GNN分别在每个节点上传播，忽略了节点之间输入的顺序。换而言之，GNN的输出是不随节点的输入顺序为转移的。另外，图中的边表示两个节点之间的依赖关系。在传统的神经网络中，这种依赖关系只能通过节点的特征表示来体现。GNN可以依赖周围的状态来更新节点的状态。最后是推理能力，与人类从日常经验中获取推理能力相似，GNN能够从非结构化数据（例如:场景图片、故事片段等）中生成图。与之对比的是，传统CNN和RNN能够从大量经验数据中生成完整的图片和文档，但并不能学习出这种推理图（reasoning graph）。

神经网络是一种可以通过训练来识别各种模式的模型。神经网络由多个层组成，包括输入层和输出层，以及至少一个隐藏层 。各层中的神经元会学习越来越抽象的数据表示法。

神经网络是通过梯度下降法进行训练的。每层的权重都以随机值开始，并且这些权重会随着时间的推移以迭代的方式不断改进，使网络更准确。我们使用损失函数量化网络的不准确程度，并使用一种名为“反向传播算法”的流程确定每个权重应该增加还是降低以减小损失。

#### 图卷积神经网络
Graph Convolutional Neural Network，GCNN


#### 门控图神经网络
Gated Graph Neural Network，GGNN

使用GRU或者是LSTM这种带有门控机制的网络，来增强信息在图结构中的长期的传播。





（1）结构化场景：数据包含有很明确的关系结构，如物理系统、分子结构和知识图谱。
（2）非结构化场景：数据不包含明确的关系结构，例如文本和图像等领域。
（3）其他应用场景：例如生成式模型和组合优化模型。



GNN被广泛的应用到包括应用物理、分子化学和知识图谱等领域。


在图像分类中，每个类别被视为图中的一个节点。


GNN还可以被应用到许多其他情景中去。

### OPEN PROBLEMS
- 浅层结构：目前GNN还只能在较浅层的网络上发挥优势，随着层数的加深，网络会出现退化。
- 动态图：目前大多方法只能应用在静态图上，对于动态图还没有特别好的解决方案。
- 非结构化场景：还没有一个通用的方法来合理的处理非结构化数据。
- 扩展性：将图网络应用于大规模数据上仍然面临着不小的困难。


