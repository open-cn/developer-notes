## tez

Tez是Apache开源的支持DAG作业的计算框架，它直接源于MapReduce框架，核心思想是将Map和Reduce两个操作进一步拆分，即Map被拆分成Input、Processor、Sort、Merge和Output， Reduce被拆分成Input、Shuffle、Sort、Merge、Processor和Output等，这样，这些分解后的元操作可以任意灵活组合，产生新的操作，这些操作经过一些控制程序组装后，可形成一个大的DAG作业。

总结起来，Tez有以下特点：

（1）Apache二级开源项目
（2）运行在YARN之上
（3）适用于DAG（有向图）应用（同Impala、Dremel和Drill一样，可用于替换Hive/Pig等）

Tez产生的主要原因是绕开MapReduce所施加的限制。除了必须要编写Mapper和Reducer的限制之外，强制让所有类型的计算都满足这一范例还有效率低下的问题。

例如使用HDFS存储多个MR作业之间的临时数据，这是一个负载。在Hive中，查询需要对不相关的key进行多次shuffle操作的场景非常普遍，例如join - group by - window function - order by。

MR性能差，资源消耗大，如：Hive作业之间的数据不是直接流动的，而是借助HDFS作为共享数据存储系统，即一个作业将处理好的数据写入HDFS，下一个作业再从HDFS重新读取数据进行处理。很明显更高效的方式是，第一个作业直接将数据传递给下游作业。总的来说之前mapReduce在map和reduce阶段都会产生I/O落盘，但是Tez就不要这一步骤了。

Tez与oozie不同：oozie只能以MR任务为整体来管理、组织，本质上仍然是多个MR任务的执行，不能解决上面提到的多个任务之间硬盘IO冗余的问题。Tez只是一个Client，部署很方便。


传统的MR（包括Hive，Pig和直接编写MR程序）。假设有四个有依赖关系的MR作业（1个较为复杂的Hive SQL语句或者Pig脚本可能被翻译成4个有依赖关系的MR作业）或者用Oozie描述的4个有依赖关系的作业，运行过程

Reduce Task，需要写HDFS

云状表示写屏蔽（write barrier，一种内核机制，持久写）

Tez可以将多个有依赖的作业转换为一个作业（这样只需写一次HDFS，且中间节点较少），从而大大提升DAG作业的性能。

spark号称比mr快100倍，而tez也号称比mr快100倍；二者性能都远超MR。

spark与tez都是以dag方式处理数据

spark更像是一个通用的计算引擎，提供内存计算，实时流处理，机器学习等多种计算方式，适合迭代计算。

tez作为一个框架工具，特定为hive和pig提供批量计算。

spark属于内存计算，支持多种运行模式，可以跑在standalone，yarn上；而tez只能跑在yarn上；虽然spark与yarn兼容，但是spark不适合和其他yarn应用跑在一起。

tez能够及时的释放资源，重用container，节省调度时间，对内存的资源要求率不高；而spark如果存在迭代计算时，container一直占用资源；

tez与spark两者并不矛盾，不存在冲突，在实际生产中，如果数据需要快速处理而且资源充足，则可以选择spark；如果资源是瓶颈，则可以使用tez；这个总结同样也适合spark与mr的比较；








### Tez原理

Tez包含的组件：

- 有向无环图（DAG）——定义整体任务。一个DAG对象对应一个任务。
- 节点（Vertex）——定义用户逻辑以及执行用户逻辑所需的资源和环境。一个节点对应任务中的一个步骤。
- 边（Edge）——定义生产者和消费者节点之间的连接。

边需要分配属性，对Tez而言这些属性是必须的，有了它们才能在运行时将逻辑图展开为能够在集群上并行执行的物理任务集合。

下面是一些这样的属性：

- 数据移动属性，定义了数据如何从一个生产者移动到一个消费者。
- 调度（Scheduling）属性（顺序或者并行），帮助我们定义生产者和消费者任务之间应该在什么时候进行调度。
- 数据源属性（持久的，可靠的或者暂时的），定义任务输出内容的生命周期或者持久性，让我们能够决定何时终止。

该模型所有的输入和输出都是可插拔的。为了方便，Tez使用了一个基于事件的模型，目的是为了让任务和系统之间、组件和组件之间能够通信。事件用于将信息（例如任务失败信息）传递给所需的组件，将输出的数据流（例如生成的数据位置信息）传送给输入，以及在运行时对DAG执行计划做出改变等。Tez还提供了各种开箱即用的输入和输出处理器。这些富有表现力的API能够让更高级语言（例如Hive）的编写者很优雅地将自己的查询转换成Tez任务。

#### TEZ技术

Application Master Pool 初始化AM池。Tez先将作业提交到AMPoolServer服务上。AMPoolServer服务启动时就申请多个AM，Tez提交作业会优先使用缓冲池资源。

Container Pool AM启动时会预先申请多个Container。

Container重用

#### TEZ实现方法

Tez对外提供了6种可编程组件，分别是：

Input：对输入数据源的抽象，它解析输入数据格式，并吐出一个个Key/value

Output：对输出数据源的抽象，它将用户程序产生的Key/value写入文件系统

Paritioner：对数据进行分片，类似于MR中的Partitioner

Processor：对计算的抽象，它从一个Input中获取数据，经处理后，通过Output输出

Task：对任务的抽象，每个Task由一个Input、Ouput和Processor组成

Maser ：管理各个Task的依赖关系，并按顺依赖关系执行他们

除了以上6种组件，Tez还提供了两种算子，分别是Sort（排序）和Shuffle（混洗），为了用户使用方便，它还提供了多种Input、Output、Task和Sort的实现

##### DAG

Edge：定义了上下游Vertex之间的连接方式。

Edge相关属性：

- Data movement：定义了producer与consumer之间数据流动的方式。
- One-To-One: 第i个producer产生的数据，发送给第i个consumer。这种上下游关系属于Spark的窄依赖。
- Broadcast: producer产生的数据路由都下游所有consumer。这种上下游关系也属于Spark的窄依赖。
- Scatter-Gather: producer将产生的数据分块，将第i块数据发送到第i个consumer。这种上下游关系属于Spark的宽依赖。

- Scheduling：定义了何时启动consumer Task
- Sequential: Consumer task 需要producer task结束后启动，如：MR。
- Concurrent: Consumer task 与producer task一起启动，如：流计算。

- Data source：定义了任务outp的生命周期与可靠性。
- Persisted: 当任务退出后，该任务output依然存在，但经过一段时间后，可能会被删除，如：Mapper输出的中间结果。
- Persisted-Reliable: 任务output总是存在，比如，MR中reducer的输出结果，存在HDFS上。
- Ephemeral: 任务输出只有当该task在运行的时候，才存在，如：流计算的中间结果。

Task是Tez的最小执行单元，Vertex中task的数量与该vertex的并行度一致。以下是Input、Processor、Output均需要实现的接口：

```java
List<Event> initialize(Tez*Context) // This is where I/P/O receive their corresponding context objects. They can, optionally, return a list of events.

handleEvents(List<Event> events) // Any events generated for the specific I/P/O will be passed in via this interface. Inputs receive DataMovementEvent(s) generated by corresponding Outputs on this interface – and will need to interpret them to retrieve data. At the moment, this can be ignored for Outputs and Processors.

List<Event> close() // Any cleanup or final commits will typically be implemented in the close method. This is generally a good place for Outputs to generate DataMovementEvent(s). More on these events later.
```

Input: 接收上游Output事件，获取上游数据位置；从physical Edge中获取实际数据；解析实际数据，为Processor提供统一的逻辑试图；

Processor: 利用Input获取实际数据，执行用户逻辑，最后输出；

Output: 将Processor提供的数据，进行分区；向下游Input发送事件；

Tez的事件驱动机制: Tez中各个组件通过不同类型的Event进行通信。

数据传输：Output通过ShuffleEvent传递上游数据位置，AM负责将Event路由到相应Input中。

容错：Input当无法获取到上游数据时，会通知框架重新调度上游任务，这也意味着任务成功完成后，仍然会被重新调度。

runtime执行计划优化：根据上游Map Stage产生的数据大小，动态reducer并行度。Output产生的事件路由到可拔插的Vertex/Edge management module，对应moudule就可以对runtime执行计划进行调整。


### tez 配置

```tez-site.xml
tez.runtime.pipelined.sorter.lazy-allocate.memory true
tez.lib.uris.classpath ./tez/tez-0.9.2-1.2.5/*,./tez/tez-0.9.2-1.2.5/lib/*
tez.task.launch.env LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/hadoop-current/lib/native
tez.use.cluster.hadoop-libs true
tez.runtime.convert.user-payload.to.history-text true
tez.am.view-acls *
tez.runtime.unordered.output.buffer.size-mb 579
tez.vertex.failures.maxpercent 0.0f
tez.container.max.java.heap.fraction 0.8
tez.staging-dir /tmp/tez/staging
tez.history.logging.service.class org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService
tez.am.speculation.enabled true
tez.runtime.compress true
tez.am.resource.memory.mb 2048
tez.tez-ui.history-url.base http://emr-header-1:8090/tez-ui2/
tez.am.maxtaskfailures.per.node 3
tez.lib.uris hdfs://emr-header-1:9000/apps/tez-0.9.2-1.2.5/tez-0.9.2-1.2.5.tar.gz#tez
tez.runtime.io.sort.mb 1158
tez.am.java.opts -XX:ParallelGCThreads=2 -XX:CICompilerCount=2
tez.am.launch.env LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/hadoop-current/lib/native
tez.runtime.compress.codec org.apache.hadoop.io.compress.SnappyCodec
```