## ML
机器学习

人类擅长并行处理（模式识别），但不擅长顺序处理（逻辑验证），而机器正好相反。人类看一眼就知道眼前是一只猫，而机器需学习上千万张猫的图片来提取特征，才能进行判断。

### 学习策略
粗略地说，有三种学习策略：

1. 监督学习<br>
最简单的方法。在已知结果的（足够大的）测试数据集之下使用。学习过程如下：

    1. 首先对数据集进行处理。
    2. 将输出与已知结果进行比较。
    3. 调整神经网络并重复测试。

这是我们将在这里使用的学习策略。

2. 无监督的学习<br>
如果没有现成的测试数据，并且可以从所需的行为中获得某种成本函数，则非常有用。成本函数告诉神经网络离目标有多远。然后，网络可以在处理实际数据时动态调整其参数。

3. 强化学习<br>
'胡萝卜加大棒'的方法。如果神经网络产生连续动作，可以使用这个方法。用奖励来激励网络向正确的方向调整，如果走错了路则会受到惩罚。慢慢的网络会倾向于选择正确的动作，并避免错误的行为。



#### Artificial Neural Network
人工神经网络 (ANN)是指由大量的处理单元(神经元) 互相连接而形成的复杂网络结构，是对人脑组织结构和运行机制的某种抽象、简化和模拟。ANN 以数学模型模拟神经元活动，是基于模仿大脑神经网络结构和功能而建立的一种信息处理系统。

ANN 不需要知道输入输出之间的确切关系，不需大量参数，只需要知道引起输出变化的非恒定因素，即非常量性参数。因此与传统的数据处理方法相比，神经网络技术在处理模糊数据、随机性数据、非线性数据方面具有明显优势，对规模大、结构复杂、信息不明确的系统尤为适用。

由 Minsley 和 Papert 提出的多层前向神经元网络(也称多层感知器)是目前最为常用的网络结构。

人工神经网络具有自学习、自组织、自适应以及很强的非线性函数逼近能力，拥有强大的容错性。它可以实现仿真、二值图像识别、预测以及模糊控制等功能。

在运用人工手段模仿人类智能行为的研究上有两种主导思想，即结构主义和功能主义。功能主义成了传统人工智能理论的研究基础。结构主义从分析人脑神经网络的微观结构入手，抓住人脑结构的主要特征，即简单的非线性神经元之间复杂而又灵活的连接关系，深刻揭示了人脑认识过程，创立了人工神经网络的理论。

人工神经元最简单模型－感知机，给平面中的点分类。

ANN 已经成功应用于许多领域的问题：

- 通过模式识别对数据进行分类。“ 这张照片上有一棵树吗？”
- 当测试数据与通常模式不匹配时，检测异常或新奇。 “卡车司机是否有可能入睡？”　“这些地震事件是正常地面运动还是大地震的预兆？”
- 通过过滤，分离或压缩等来处理信号。
- 估测目标函数 - 对预测和预报很有用。 “这场风暴会变成龙卷风吗？”

- 人脸识别
- 语音识别
- 手写体识别
- 文本翻译
- 智力游戏（通常是棋盘游戏或纸牌游戏）
- 自动驾驶汽车和机器人
- 还有太多应用！

##### 神经元：构建神经网络的基本模块
任何人工神经网络的基本成分是人工神经元。 它们不仅以其生物学对应物命名，而且还以我们大脑中神经元的行为为模型。

就像生物神经元具有接收信号的树突，处理信号的细胞体，以及将信号发送到其他神经元的轴突一样，人工神经元具有多个输入通道，信号处理单元和一个可以通向众多人工神经单元的输出通道。

神经元内部的计算实际上是多么简单。 确定地有以下三个处理步骤：

1. 每个输入信号都按一定比例放大或缩小

    当信号进入时，它会乘以分配给该特定输入的权重值。也就是说，如果神经元有三个输入，那么它们三个的权重都可以单独调整。在学习期间，神经网络可以基于最后测试结果的误差来调整权重。

2. 累加所有信号

    在这一步骤中，将修正过的输入信号相加得到单个值(总和）。在此步骤中，还要给总和添加偏移量。这种偏移称为偏差(bias)。神经网络还在学习中调整偏差。

    神奇的事情就是在这个时候发生的！一开始，所有神经元都被赋予随机权重和随机偏差。在每次学习迭代之后，权重和偏差逐渐位移，以使下一次结果更接近期望的输出。这样，神经网络逐渐习得我们所需要的模型状态。

3. 激活

    最后，神经元计算的结果变成输出信号。这是通过将结果馈送到激活函数（也称为传递函数）来完成的。

##### 激活函数
在计算机网络中，一个节点的激活函数定义了该节点在给定的输入或输入的集合下的输出。标准的计算机芯片电路可以看作是根据输入得到开（1）或关（0）输出的数字电路激活函数。这与神经网络中的线性感知机的行为类似。然而，只有非线性激活函数才允许这种网络仅使用少量节点来计算非平凡问题。 在人工神经网络中，这个功能也被称为传递函数。

Sigmoid函数是一个在生物学中常见的S型函数，也称为S型生长曲线。在信息科学中，由于其单增以及反函数单增等性质，Sigmoid函数常被用作神经网络的激活函数，将变量映射到0,1之间。

sigmoid函数也叫Logistic函数，用于隐层神经元输出，取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好。Sigmoid作为激活函数有以下优缺点：
优点：平滑、易于求导。
缺点：激活函数计算量大，反向传播求误差梯度时，求导涉及除法；反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练。

sigmoid函数和tanh函数是研究早期被广泛使用的2种激活函数。两者都为S 型饱和函数。 当sigmoid 函数输入的值趋于正无穷或负无穷时，梯度会趋近零，从而发生梯度弥散现象。sigmoid函数的输出恒为正值，不是以零为中心的，这会导致权值更新时只能朝一个方向更新，从而影响收敛速度。tanh 激活函数是sigmoid 函数的改进版，是以零为中心的对称函数，收敛速度快，不容易出现 loss 值晃动，但是无法解决梯度弥散的问题。2个函数的计算量都是指数级的，计算相对复杂。softsign 函数是 tanh 函数的改进版，为 S 型饱和函数，以零为中心，值域为（−1，1）。

损失函数与随机梯度下降方法

反向传播与梯度下降

链式求导法则

过拟合问题
一个好的经验法则是，训练数据点的总数至少应该是神经网络中参数数的2到3倍，尽管数据样本的精确数量取决于手头的特定模型。一般来说，参数较多的模型被认为具有较高的容量，为了获得对未知测试数据的泛化能力，模型需要更多的数据。在机器学习中，过拟合的概念常被理解为偏差和方差之间的权衡。


##### 感知机 Perceptron
激活函数的最基本形式是一个简单的二元方程，这个方程只有两个可能的结果。

尽管看起来如此简单，但该函数有一个非常复杂的名称：Heaviside Step 函数。 如果输入为正或零，则此函数返回1;对于任何负输入，此函数返回0。 使用这样的激活函数的神经元被称为感知机。

神经网络的基础模型是感知机，因此神经网络也可以叫做多层感知机(Multi-layer Perceptron)，简称MLP。单层感知机叫做感知机，多层感知机(MLP)≈人工神经网络(ANN)。

#### CNN
卷积神经网络，CNN

#### RNN
递归神经网络，RNN

#### Deep Neural Network
深度神经网络，DNN


















